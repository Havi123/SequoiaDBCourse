---
show: step
version: 1.0
enable_checker: true
---

# 多实例数据共享简介

在本例中将试验多个实例共存的情况，包括：mysql，postgresql，sparksql实例。
在第上个示例中我们创建了sparksql实例，并使用mysql实例存储元数据；在本例中将增加一个postgresql实例，并且演示在MySQL实例中插入数据，
然后通过postgresql和sparksql进行数据分析的过程。

### 1 准备课程环境

课程环境是一个docker 容器，已经安装了一个单副本的巨杉数据库，包括了多种实例的安装介质，可以在这容器中完成多种实例的安装配置。 
进入Container，安装postgresql实例（前面一次测试已经安装了mysql和sparksql实例）
在属主机上运行：

```
docker start sdbtestfu
docker attach sdbtestfu
```
在Container中，用root执行
```
cd /sequoiadb/sequoiadb-3.2.4
./sequoiasql-postgresql-3.2.4-x86_64-installer.run
```
使用缺省安装参数。

用sdbadmin用户创建实例并启动所有实例。
```
su - sdbadmin
cd /opt/sequoiasql/postgresql/bin
创建PostgreSQL实例：
./sdb_sql_ctl addinst myinst -D database/5432/
```

启动所有的实例进程：

1.启动sdb进程
```
sdbstart -t all
```
2.启动postgresql
```
cd /opt/sequoiasql/postgresql/bin
./sdb_sql_ctl start myinst
```
3.启动mysql实例
```
cd /opt/sequoiasql/mysql/bin
./sdb_sql_ctl start myinst
```
4.启动sparksql实例
root用户执行：
```
/etc/init.d/ssh start
```
sdbadmin用户执行：
```
su - sdbadmin
cd /opt/spark-2.1.3-bin-hadoop2.7/sbin
 ./start-all.sh
/opt/spark-2.1.3-bin-hadoop2.7/sbin/start-thriftserver.sh --master spark://sdb:7077
```
5.检查实例进程是否启动
```
netstat -an |grep LISTEN |grep 3306
netstat -an |grep LISTEN |grep 5432
netstat -an |grep LISTEN |grep 7077
netstat -an |grep LISTEN |grep 10000 
```
## 2 通过mysql创建测试表，并插入数据
通过MySQL实例创建两张表：
表1：prod_tab
字段定义：
prod_name varchar(50),prod_id int,prod_place varchar(100)

表2：sale_tab
字段定义：
prod_id int,prod_volume int,sale_quantity double

```
mysql -h127.0.0.1 -uroot -p

create database testsdb;
use testsdb;
create table prod_tab(prod_name varchar(50),prod_id int,prod_place varchar(100));
create table sale_tab(prod_id int,prod_volume int,sale_quantity double);
```
在prod_tab 中插入10个产品信息：
```
insert into prod_tab values("prod1",1,"chengdu1");
insert into prod_tab values("prod2",2,"chengdu2");
insert into prod_tab values("prod3",3,"chengdu3");
insert into prod_tab values("prod4",4,"chengdu4");
insert into prod_tab values("prod5",5,"chengdu5");
insert into prod_tab values("prod6",6,"chengdu6");
insert into prod_tab values("prod7",7,"chengdu7");
insert into prod_tab values("prod8",8,"chengdu8");
insert into prod_tab values("prod9",9,"chengdu9");
insert into prod_tab values("prod10",10,"chengdu10");
```
在sale_tab中插入 24条记录
```
insert into sale_tab values(1,10,100);
insert into sale_tab values(2,20,200);
insert into sale_tab values(3,30,300);
insert into sale_tab values(4,40,400);
insert into sale_tab values(5,50,500);
insert into sale_tab values(6,60,600);
insert into sale_tab values(7,70,700);
insert into sale_tab values(8,80,800);
insert into sale_tab values(9,90,900);
insert into sale_tab values(1,20,200);
insert into sale_tab values(2,10,100);
insert into sale_tab values(3,40,400);
insert into sale_tab values(1,10,100);
insert into sale_tab values(1,40,400);
insert into sale_tab values(10,100,1000);
insert into sale_tab values(3,10,100);
insert into sale_tab values(7,10,100);
insert into sale_tab values(6,10,100);
insert into sale_tab values(5,10,100);
insert into sale_tab values(8,10,100);
insert into sale_tab values(5,50,500);
insert into sale_tab values(10,10,100);
insert into sale_tab values(4,10,100);
insert into sale_tab values(1,10,100);
```
查看数据：
```
mysql> select * from sale_tab;
mysql> select * from prod_tab;
mysql> quit；
```

在sdb中查看数据：
```
sdb
> db=new Sdb()
localhost:11810
Takes 0.001151s.
> db.list(4)
{
  "Name": "sparkCS.sparkCL"
}
{
  "Name": "testsdb.prod_tab"
}
{
  "Name": "testsdb.sale_tab"
}
Return 3 row(s).
Takes 0.000916s.
> db.testsdb.prod_tab.find()
> db.testsdb.sale_tab.find()
```
## 3 把这两张表映射为postgresql的外表：

1.创建postgre 数据库,数据库名称与MySQL中的相同。
```
cd /opt/sequoiasql/postgresql/bin
./sdb_sql_ctl createdb testsdb myinst
```
2.进入 SequoiaSQL PostgreSQL shell 环境
```
./psql -p 5432 testsdb
```
3.加载SequoiaDB连接驱动
```
pgsdb=# create extension sdb_fdw;
```
配置与SequoiaDB连接参数
```
pgsdb=# create server sdb_server foreign data wrapper sdb_fdw options(address '127.0.0.1', service '11810', preferedinstance 'A', transaction 'off');
```
4.映射sdb表为postgresql的外表
```
create foreign table prod_tab (prod_name text,prod_id int,prod_place text) server sdb_server options ( collectionspace 'testsdb', collection 'prod_tab', decimal 'on');
CREATE FOREIGN TABLE
create foreign table sale_tab (prod_id int,prod_volume int,sale_quantity real) server sdb_server options ( collectionspace 'testsdb', collection 'sale_tab', decimal 'on');

CREATE FOREIGN TABLE
testsdb=# \d
              List of relations
 Schema |   Name   |     Type      |  Owner   
--------+----------+---------------+----------
 public | prod_tab | foreign table | sdbadmin
 public | sale_tab | foreign table | sdbadmin
(2 rows)
```
更新表的统计信息
```
pgsdb=# analyze prod_tab;
ANALYZE
testsdb=# analyze sale_tab;    
ANALYZE
```
检查表：
```
pgsdb=# \d
```
5.在postgresql中执行操作
```
testsdb=# select * from prod_tab where prod_id=3;
 prod_name | prod_id | prod_place 
-----------+---------+------------
 prod3     |       3 | chengdu3
(1 row)
testsdb=# select a.prod_name,a.prod_id,a.prod_place,b.prod_volume,b.sale_quantity from prod_tab a,sale_tab b where a.prod_id=b.prod_id;
testsdb=# select a.prod_id,sum(b.prod_volume) total_volume,sum(b.sale_quantity) total_quantity from prod_tab a,sale_tab b where a.prod_id=b.prod_id group by a.prod_id;
```

退出psql
```
\q
```
## 4 在sparksql实例中访问数据
1.关联sdb表与sparksql表
```
cd /opt/spark-2.1.3-bin-hadoop2.7/bin
/opt/spark-2.1.3-bin-hadoop2.7/bin/beeline -u jdbc:hive2://sdb:10000 -n sdbadmin -p sdbadmin
Connecting to jdbc:hive2://sdb:10000
log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Connected to: Spark SQL (version 2.1.3)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 1.2.1.spark2 by Apache Hive
0: jdbc:hive2://sdb:10000> 
```

关联sdb中的集合空间和集合，为spark中的一张表：
```
CREATE table prod_tab (prod_name string,prod_id int,prod_place string) using com.sequoiadb.spark OPTIONS ( host 'sdb:11810', collectionspace 'testsdb', collection 'prod_tab');

CREATE table sale_tab (prod_id int,prod_volume int,sale_quantity double) using com.sequoiadb.spark OPTIONS ( host 'sdb:11810', collectionspace 'testsdb', collection 'sale_tab');
```
2.在sparksql中操作数据
```
select a.prod_name,a.prod_id,a.prod_place,b.prod_volume,b.sale_quantity from prod_tab a,sale_tab b where a.prod_id=b.prod_id;
select a.prod_id,sum(b.prod_volume) total_volume,sum(b.sale_quantity) total_quantity from prod_tab a,sale_tab b where a.prod_id=b.prod_id group by a.prod_id;
```
3.退出beeline
```
0: jdbc:hive2://sdb:10000> !quit
```

## 结束课程

退出Container
```
$exit
#exit
```
在属主机中，删除Container
```
docker rm sdbtestfu
```
