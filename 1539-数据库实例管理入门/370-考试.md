---
version: 69.0
---

## 考试介绍

此考试是在完成部署 SequoiaDB 巨杉数据库引擎的环境中完成 MySQL、PostgreSQL、SparkSQL 实例的创建以及使用。

SequoiaDB巨杉数据库采用“计算存储分离”架构，支持 MySQL、PostgreSQL 与 SparkSQL 三种关系型数据库实例、类 MongoDB 的 JSON 文档类数据库实例、以及 S3 对象存储与 Posix 文件系统的非结构化数据实例。在事务场景可以利用 SDBAPI、MySQL 和 PGSQL 实例对数据进行操作，在分析场景借助分布式计算框架 Spark 的并发计算性能，提高计算效率。

> Note:
> - 考试内容需在`sdbadmin`系统用户下完成，用户密码为 `sdbadmin`
> - SequoiaDB 巨杉数据库安装目录(/opt/sequoiadb)
> - SequoiSQL-MySQL 安装目录(/opt/sequoiasql/mysql)
> - SequoiSQL-PostgreSQL 安装目录(/opt/sequoiasql/postgresql)

#### 知识点

MySQL，PostgreSQL，SparkSQL实例的创建以及使用过程。

## 考试内容

1）创建一个 MySQL 实例，实例名为 myinst，使用的端口为 3306；

2）创建 metauser 用户，用于创建 SparkSQL 元数据库 metastore，存放元数据信息；

> Note:
>
> 保持 MySQL 实例用户 root 密码为空

2）在 MySQL 实例中创建数据库 company，数据表 employee ( empno INT, ename VARCHAR(128), age INT )，并写入如下数据，然后查询是否存在数据；

 - (empno:10001, ename:'Georgi', age:48)

3）创建一个 PostgreSQL 实例，实例名为 pginst，使用的端口为 5432，并与 SequoiaDB 巨杉数据库对接；

4）在 PostgreSQL 实例中创建数据库 company，外部表 employee 与 MySQL 实例创建的分区表对应，然后查询是否存在数据；

5）安装部署 SparkSQL 实例，配置 SparkSQL 的元数据信息到 MySQL 实例中，要求安装后 SPARK 的 HOME 目录为：/home/sdbadmin/spark-2.4.4-bin-hadoop2.7

> Note: SparkSQL 实例的安装包目录：/home/sdbadmin/soft 

6）SparkSQL 实例中创建数据库 company 和数据表 employee 与 MySQL 实例中创建的数据库 company 和数据表 employee 对应，实现数据共享；

> Note:
>
> 使用 spark-sql 客户端进行 SparkSQL 的操作


## 考试要求

1）创建 MySQL，PostgreSQL，SparkSQL实例；

2）实现 MySQL，PostgreSQL，SparkSQL 实例的数据共享；



## 提示

1）对接过程中使用到的驱动包：SequoiaDB for Spark 连接器 spark-sequoiadb_2.11-3.4.jar，SequoiaDB 的 java 驱动 sequoiadb-driver-3.4.jar，MySQL 的 java 驱动 mysql-jdbc.jar；


> Note:
> 
> - spark-sequoiadb_2.11-3.4.jar 所在目录为 `/opt/sequoiadb/spark/spark-sequoiadb_2.11-3.4.jar`
> - sequoiadb-driver-3.4.jar 所在目录为 `/opt/sequoiadb/java/sequoiadb-driver-3.4.jar`
> - mysql-jdbc.jar 所在目录为 `/home/sdbadmin/soft/mysql-jdbc.jar`


2）在 MySQL 实例中，创建用于存储 SparkSQL 的元数据信息的库 metastore，编码设置为 latin1，排序规则类型为 latin1_bin，以下是创建 metastore 的语句；

```sql
CREATE DATABASE metastore CHARACTER SET 'latin1' COLLATE 'latin1_bin' ;
```


3）配置 Spark 的元数据库用到的 hive-site.xml 文件的内容；

```xml
<configuration>
   <property>
     <name>hive.metastore.schema.verification</name>
     <value>false</value>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://localhost:3306/metastore?useSSL=false</value>
      <description>JDBC connect string for a JDBC metastore</description>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
      <description>Driver class name for a JDBC metastore</description>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>metauser</value>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>metauser</value>
   </property>
   <property>
      <name>datanucleus.autoCreateSchema</name>
      <value>true</value>
      <description>creates necessary schema on a startup if one doesn't exist. set this to false, after creating it once</description>
   </property>
</configuration>
```



<!--

5）解压 Spark 安装包，解压路径为 /home/sdbadmin/spark-2.4.4-bin-hadoop2.7 ；

> Note:
> 
> SparkSQL 实例的安装包在 /home/sdbadmin/soft 目录

6）在 MySQL 实例中，创建用于存储 SparkSQL 的元数据信息的库 metastore，编码设置为 latin1，排序规则类型为 latin1_bin，以下是创建 metastore 的语句；

```sql
CREATE DATABASE metastore CHARACTER SET 'latin1' COLLATE 'latin1_bin' ;
```

7）创建 metauser 用户，密码为 metauser；

8）赋予 metauser 所有权限；

9）配置 sdbadmin 用户 ssh 免密；

10）配置 spark-env.sh ；

11）配置 Spark 的元数据库，以下是 hive-site.xml 文件的内容；

```xml
<configuration>
   <property>
     <name>hive.metastore.schema.verification</name>
     <value>false</value>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://localhost:3306/metastore?useSSL=false</value>
      <description>JDBC connect string for a JDBC metastore</description>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
      <description>Driver class name for a JDBC metastore</description>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>metauser</value>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>metauser</value>
   </property>
   <property>
      <name>datanucleus.autoCreateSchema</name>
      <value>true</value>
      <description>creates necessary schema on a startup if one doesn't exist. set this to false, after creating it once</description>
   </property>
</configuration>
```

12）拷贝 SequoiaDB for Spark 连接器 spark-sequoiadb_2.11-3.4.jar，SequoiaDB 的 java 驱动 sequoiadb-driver-3.4.jar，MySQL 的 java 驱动 mysql-jdbc.jar；

> Note:
> 
> - spark-sequoiadb_2.11-3.4.jar 所在目录为 `/opt/sequoiadb/spark/spark-sequoiadb_2.11-3.4.jar`
> - sequoiadb-driver-3.4.jar 所在目录为 `/opt/sequoiadb/java/sequoiadb-driver-3.4.jar`
> - mysql-jdbc.jar 所在目录为 `/home/sdbadmin/soft/mysql-jdbc.jar`

13）启动 Spark；

14）在 SparkSQL 中创建数据库名为 company，表名为 employee，然后查询是否存在数据；

> Note:
>
> 使用 spark-sql 客户端进行 SparkSQL 的操作


## 示例代码

#### 切换用户

```shell
su - sdbadmin
```

> Note:
>
> sdbadmin 用户的密码为 sdbadmin

#### MySQL实例

1）创建 myinst 实例，监听的端口为 3306；

```shell
/opt/sequoiasql/mysql/bin/sdb_sql_ctl addinst myinst -D /opt/sequoiasql/mysql/database/3306 -p 3306
```

2）登录 MySQL Shell；

```shell
/opt/sequoiasql/mysql/bin/mysql -h127.0.0.1 -P3306 -uroot 
```

3）创建 company 数据库，并切换至该数据库；

```sql
CREATE DATABASE company ;
USE company ;
```

4）创建 employee 数据表；

```sql
CREATE TABLE employee (empno INT, ename VARCHAR(128), age INT) ;
```

5）插入数据；

```sql
INSERT INTO employee (empno, ename, age) VALUES (10001, "Georgi", 48) ;
```

6）查询数据是否存在；

```sql
SELECT * FROM employee;
```

7）退出 MySQL Shell

```sql
\q
```

#### PostgreSQL实例

1）创建 PostgreSQL 实例；

```shell
/opt/sequoiasql/postgresql/bin/sdb_sql_ctl addinst pginst -D /opt/sequoiasql/postgresql/database/5432 -p 5432
```

2）启动 pginst 实例；

```shell
/opt/sequoiasql/postgresql/bin/sdb_sql_ctl start pginst
```

3）创建 company 库；

```shell
/opt/sequoiasql/postgresql/bin/sdb_sql_ctl createdb company pginst
```

4）进入 PostgreSQL shell；

```shell
/opt/sequoiasql/postgresql/bin/psql -p 5432 company
```

5）加载SequoiaDB连接驱动；

```sql
CREATE EXTENSION sdb_fdw ;
```

6）配置与SequoiaDB连接参数；

```sql
CREATE SERVER sdb_server FOREIGN DATA WRAPPER sdb_fdw 
OPTIONS (address '127.0.0.1', service '11810', preferedinstance 'A', transaction 'on') ;
```

7）创建 company 数据库外表；

```sql
CREATE FOREIGN TABLE employee (
  empno INTEGER, 
  ename TEXT,
  age INTEGER
) SERVER sdb_server 
OPTIONS (collectionspace 'company', collection 'employee', decimal 'on') ;
```

8）更新表的统计信息；

```sql
ANALYZE employee ;
```

9）查询 employee 表中的数据；

```sql
SELECT * FROM employee ;
```

10）退出 PostgreSQL shell；

```sql
\q
```

#### SparkSQL实例

1）使用 MySQL Shell 连接 SequoiaDB-MySQL 实例；

```shell
/opt/sequoiasql/mysql/bin/mysql -h 127.0.0.1 -P 3306 -u root
```

2）创建 metauser 用户；

```sql
CREATE USER 'metauser'@'%' IDENTIFIED BY 'metauser' ;
```

3）给 metauser 用户授权；

```sql
GRANT ALL ON *.* TO 'metauser'@'%' ;
```

4）刷新权限；

```sql
FLUSH PRIVILEGES ;
```

5）创建元数据库 metasore；

```sql
CREATE DATABASE metastore CHARACTER SET 'latin1' COLLATE 'latin1_bin' ;
```

6）退出 MySQL Shell；

```sql
\q
```

7）进入安装包存放目录；

```shell
cd /home/sdbadmin/soft
```

8）解压安装包；

```shell
tar -zxvf spark-2.4.4-bin-hadoop2.7.tar.gz -C /home/sdbadmin
```

9）执行ssh-keygen生成公钥和密钥，执行后连续回车即可；

```shell
ssh-keygen -t rsa
```

10）执行 ssh-copy-id，把公钥拷贝到本机的 sdbadmin 用户；

```shell
ssh-copy-id  sdbadmin@`hostname`
```

>
>Note:
>
> 用户 sdbadmin 的密码是：`sdbadmin`


12）进入 Spark 的配置目录；

```shell
cd /home/sdbadmin/spark-2.4.4-bin-hadoop2.7/conf
```

13）从模板中拷贝 spark-env.sh 文件；

```shell
cp spark-env.sh.template spark-env.sh
```

14）设置 Spark 实例的 Master；

```shell
echo "SPARK_MASTER_HOST=`hostname`" >> spark-env.sh
```

15）查看 spark-env.sh 文件是否设置成功；

```shell
cat spark-env.sh
```

16）创建设置元数据数据库配置文件 hive-site.xml；

```shell
cat > /home/sdbadmin/spark-2.4.4-bin-hadoop2.7/conf/hive-site.xml << EOF
<configuration>
   <property>
     <name>hive.metastore.schema.verification</name>
     <value>false</value>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://localhost:3306/metastore?useSSL=false</value>
      <description>JDBC connect string for a JDBC metastore</description>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
      <description>Driver class name for a JDBC metastore</description>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>metauser</value>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>metauser</value>
   </property>
   <property>
      <name>datanucleus.autoCreateSchema</name>
      <value>true</value>
      <description>creates necessary schema on a startup if one doesn't exist. set this to false, after creating it once</description>
   </property>
</configuration>
EOF
```

17）检查是否创建成功；

```shell
cat /home/sdbadmin/spark-2.4.4-bin-hadoop2.7/conf/hive-site.xml
```

18）拷贝 spark-sequoiadb.jar 驱动连接器；

```shell
cp /opt/sequoiadb/java/sequoiadb-driver-3.4.jar  /home/sdbadmin/spark-2.4.4-bin-hadoop2.7/jars/
```

19）拷贝 SequoiaDB 的 java 驱动 sequoiadb.jar；

```shell
cp /opt/sequoiadb/spark/spark-sequoiadb_2.11-3.4.jar  /home/sdbadmin/spark-2.4.4-bin-hadoop2.7/jars/
```

20）拷贝 MySQL 的 java 驱动 mysql-jdbc.jar；

```shell
cp /home/sdbadmin/soft/mysql-jdbc.jar  /home/sdbadmin/spark-2.4.4-bin-hadoop2.7/jars/  
```

21）进入 Spark 的安装目录；

```
cd /home/sdbadmin/spark-2.4.4-bin-hadoop2.7
```

22）启动 Spark；

```shell
sbin/start-all.sh
```

23）查看 Spark 的 master 和 worker 是否启动完成；

```shell
jps
```

24）配置 spark-sql 日志输出；

```shell
cat > /home/sdbadmin/spark-2.4.4-bin-hadoop2.7/conf/log4j.properties << EOF
# Set everything to be logged to the console
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

log4j.logger.org.apache.spark.repl.Main=WARN

log4j.logger.org.spark_project.jetty=WARN
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR

log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
EOF
```

25）检查日志输出配置是否成功；

```shell
cat /home/sdbadmin/spark-2.4.4-bin-hadoop2.7/conf/log4j.properties
```

26) 启动 spark-sql 客户端；

```shell
bin/spark-sql
```

27）创建 company 数据库；

```sql
CREATE DATABASE company ;
USE company ;
```

28）创建映射表；

```sql
CREATE TABLE company.employee (
empno INT,
ename STRING,
age INT
) USING com.sequoiadb.spark OPTIONS (host 'localhost:11810', collectionspace 'company', collection 'employee', username '', password '') ;
```

29）测试运行 sql；

```sql
SELECT * FROM company.employee ;
```

30）退出 spark-sql 客户端；

```sql
exit ;
```
-->
















