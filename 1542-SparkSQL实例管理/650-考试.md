# 考试

## 挑战介绍

spark作为一种快速、通用、可扩展的大数据分析引擎,是当前大数据时代中数据处理中不可缺少的大数据并行计算框架,它基于内存计算,在保证了数据处理的实时性的,同时还保证了高容错性和高可伸缩性,允许用户将spark集群部署在大量廉价的硬件之中.

本挑战使用Spark-SequoiaDB连接组件，将SequoiaDB可以作为Spark的数据源，从而可以通过SparkSQL实例对SequoiaDB数据存储引擎的数据进行查询、统计操作,从而熟悉sequoiadb和SparkSQL关联表的创建使用,spark的计划分析等等作用

#### 知识点

sequoiadb和spark表的关联,spark的计划分析

## 挑战内容

1) 在已搭建并对接好的sequoiadb和spark的环境中，在Sequoiadb创建集合空间company,集合employee,插入数据(id:10001, name:'Georgi', age:48);

2) 用beeline登录spark,创建对应集合空间的数据库company,对应集合的表employee,插入数据(id:10002, name:'Bezalel', age:21);

3) 在spark上查看数据内容是否正常;

4) 在sequoiadb上查看数据内容是否正常;

5) 执行计划分析:

## 挑战要求

1) 完成sequoiadb和spark关联表的创建;

2) 插入的两条数据要在spark以及sequoiadb正确显示;

3) 进行SparkSQL 执行计划的分析
 

## 示例代码
```
sequoiadb:
sdb 'db = new Sdb("localhost",11810)'
sdb 'db.createCS("company").createCL("employee")'
sdb 'db.company.employee.insert({id:10001, name:"Georgi", age:48})'
sdb 'db.company.employee.find()'

spark:
/opt/spark/bin/beeline -u 'jdbc:hive2://localhost:10000'
CREATE DATABASE company;
USE company;
CREATE TABLE employee (
  id  INT,
  name  VARCHAR(128),
  age    INT
) USING com.sequoiadb.spark OPTIONS (
  host 'localhost:11810',
  collectionspace 'company',
  collection 'employee'
) ;
INSERT INTO company.employee VALUES (10002, 'Bezalel', 21) ;
SELECT * FROM company.employee;
EXPLAIN SELECT * FROM company.employee;

sequoiadb:
sdb 'db = new Sdb("localhost",11810)'
sdb 'db.company.employee.find()'
```


## 挑战配置联机交易与统计分析相互隔离

#### 挑战介绍

配置 SparkSQL 实例与 MySQL 实例连接不同的协调节点，并在 SparkSQL 连接的协调节点上配置为优先从备节点读取数据。

#### 知识点

配置联机交易与统计分析相互隔离

## 挑战内容

1) 创建集合空间company,集合employee;

2) 在MySQL创建Sequoiadb映射表,并插入数据(10001, 'Georgi', 48), 
                                      (10002, 'Bezalel', 21), 
                                      (10003, 'Parto', 33), 
                                      (10004, 'Chirstian', 40),
                                      (10005, 'Kyoichi', 23), 
                                      (10006, 'Anneke', 19), 
                                      (10007, 'Ileana', 28), 
                                      (10008, 'Liz', 38), 
                                      (10009, 'Parish', 31), 
                                      (10010, 'Odette', 23）
  
3) 将协调节点21810设置为只读备节点,SparkSQL连接协调节点21810;

4) 确认数据组中11810为主节点,检查数据组1各数据节点的数据读取量

## 挑战要求

1) 配置 SparkSQL 实例与 MySQL 实例连接不同的协调节点；

2) 在 SparkSQL 连接的协调节点上配置为优先从备节点读取数据。

#### 提示

1) SparkSQL 中指定协调节点的连接是在建表的时候指定；

2) SparkSQL 建表时需要与 SequoiaDB 数据库中的集合空间和集合进行关联；

3) SparkSQL 建表时需要保证 SequoiaDB 数据库中集合空间和集合已存在；

4) SequoiaDB 数据库的读写分离可针对节点进行配置，并且在下一次连接生效。

## 示例代码

```--------------------------------------------------------
sdb 'db = new Sdb("localhost",11810)'
sdb 'dB.createCS("company").createCL("employee")'
-----------------------------------------------------------

----------------------------------------------------------
bin/sdb_sql_ctl chconf myinst --sdb-conn-addr=sdbserver1:11810
/opt/sequoiasql/mysql/bin/mysql -h 127.0.0.1 -P 3306 -u root
CREATE DATABASE company;
USE company ;
CREATE TABLE employee (
    empno INT AUTO_INCREMENT PRIMARY KEY, 
    ename VARCHAR(128), 
    age INT
) ;

INSERT INTO employee VALUES (10001, 'Georgi', 48), 
                            (10002, 'Bezalel', 21), 
                            (10003, 'Parto', 33), 
                            (10004, 'Chirstian', 40),
                            (10005, 'Kyoichi', 23), 
                            (10006, 'Anneke', 19), 
                            (10007, 'Ileana', 28), 
                            (10008, 'Liz', 38), 
                            (10009, 'Parish', 31), 
                            (10010, 'Odette', 23）;
----------------------------------------------------------

-----------------------------------------------------------
sdb 'db = new Sdb("localhost",11810)'
sdb 'db.updateConf ( { preferredinstance : "S" } , { NodeName : "sdbserver1:21810" } )'
-----------------------------------------------------------

-----------------------------------------------------------
/opt/spark/bin/beeline -u 'jdbc:hive2://localhost:10000'
CREATE TABLE employee (
  id  INT,
  name  VARCHAR(128),
  age    INT
) USING com.sequoiadb.spark OPTIONS (
  host 'localhost:21810',
  collectionspace 'company',
  collection 'employee'
) ;
--------------------------------------------------------

----------------------------------------------------------
sdb 'db.snapshot( 4, { Name : "company.employee" } )' | grep -E "NodeName|TotalDataRead|GroupName"
sdb 'db.snapshot(4, {Name: "company.employee"})' | grep -E "NodeName|TotalDataRead|GroupName"
```-----------------------------------------------------
