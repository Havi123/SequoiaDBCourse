## 挑战介绍

Spark 作为一种快速、通用、可扩展的大数据分析引擎，是当前大数据时代数据处理中不可缺少的数据并行计算框架，它基于内存计算，在保证了数据处理的实时性的同时还保证了高容错性和高可伸缩性，允许用户将 Spark 集群部署在大量廉价的硬件之中。

SequoiaDB 巨杉数据库引擎支持通过 Spark-SequoiaDB 连接组件，将 SequoiaDB 作为 Spark 的数据源，从而可以对 SequoiaDB 巨杉数据库存储引擎的数据进行查询、统计操作。

本挑战是在已搭建并对接好 SequoiaDB 巨杉数据库和 SparkSQL 实例的环境中进行如下挑战：
- 在 SequoiaDB 巨杉数据库中创建集合关联 SparkSQL 实例的映射表，进行数据查询；
- 配置 SparkSQL 实例与 MySQL 实例连接不同的协调节点，并在 SparkSQL 连接的协调节点上配置为优先从备节点读取数据。


#### 知识点

1）在 SequoiaDB 巨杉数据库引擎中创建数据域、集合空间和集合；
2）SequoiaDB 巨杉数据库中的集合和 SparkSQL 实例的表关联，分析执行计划；
3）配置联机交易与统计分析相互隔离；

## 挑战内容

1）切换到 sdbadmin 用户；

>Note:
>
>用户 sdbadmin 的密码为 `sdbadmin`
>
>部署 SequoiaDB 巨杉数据库、SparkSQL 实例的操作系统用户为 sdbadmin


2）在 SequoiaDB 数据库上创建集合空间 company，集合 employee，写入如下数据：

- ( empno : 10001, ename : 'Georgi', age : 48 )；


3）在 MySQL 实例创建数据库 company 和 数据表 employee 与 SequoiaDB 巨杉数据库存储引擎的 employee 集合映射；

4）查看当前 MySQL 实例连接 SequoiaDB 的地址和端口是否为11810；

5）使用 beeline 客户端连接 SparkSQL 的 thriftserver 服务，创建对应集合空间的数据库 company，对应集合的表 employee；建表要求：
- 连接 SequoiaDB 协调节点设置为 21810；
- 设置 SparkSQL 读取数据时优先选择备节点，降低主节点压力；


7）使用 SELECT 方法查看数据；

8）使用 explain 分析任意 SQL 的执行计划；


## 挑战要求

1）完成 SequoiaDB 和 SparkSQL 关联表的创建；

2）进行 SparkSQL 执行计划的分析；

3）配置 SparkSQL 实例与 MySQL 实例连接不同的协调节点；

4）在 SparkSQL 连接的协调节点上配置为优先从备节点读取数据；

#### 提示

1）SparkSQL 中指定协调节点的连接是在建表的时候指定；

2）SparkSQL 中与 SequoiaDB 关联的表，如果想要修改参数，可对 SparkSQL 表进行重建，并使用正确参数；

3）SparkSQL 建表时需要与 SequoiaDB 数据库中的集合空间和集合进行关联；

4）SparkSQL 建表时需要保证 SequoiaDB 数据库中集合空间和集合已存在；

5）SequoiaDB 数据库的读写分离可针对节点进行配置，并且在下一次连接生效。








<!--
测试判断:
1. 查看sdb数据是否正常; /opt/sequoiadb/bin/sdb 'db = new Sdb(); db.company.employee.find()' | grep 10001 ;SequoiaDB 数据异常

2. 查看spark数据是否正常; /opt/spark/bin/beeline -u 'jdbc:hive2://localhost:10000' -e 'SELECT * FROM company.employee where empno in ("10001", "10002");' | grep -E "10002"; Spark数据异常

示例代码：
1.
sdb 'db = new Sdb(); db.createCS("company").createCL("employee")'
sdb 'db.company.employee.insert({empno : 10001, ename : "Georgi", age : 48})'

2. 
/opt/spark/bin/beeline -u 'jdbc:hive2://localhost:10000' -e 'create database company; create table company.employee ( empno int, ename varchar(128), age int)using com.sequoiadb.spark options( host "localhost:11810", collectionspace "company", collection "employee");'

/opt/spark/bin/beeline -u 'jdbc:hive2://localhost:10000' -e 'insert into company.employee values( 10002, "Bezalel", 21);'
-->




<!--
测试判断：
1. 查看优先备节点是否设置； cat /opt/sequoiadb/conf/local/21810/sdb.conf | grep -i  "PreferInstance=S"; 协调节点 21810 优先读取备数据节点配置错误
2. 查看spark协调节点配置;/opt/spark/bin/beeline -u 'jdbc:hive2://localhost:10000' -e 'show create table company.employee' | grep 21810; spark 建表协调节点配置错误，需要指定优先读取备节点数据的协调节点哦


示例代码：
1.
sdb 'db = new Sdb(); db.createCS("company").createCL("employee")'

2.
sdb 'db.updateConf({preferinstance:"S"},{NodeName:"sdbserver1:21810"})'

/opt/spark/bin/beeline -u 'jdbc:hive2://localhost:10000' -e 'create database  if not exists company;drop table  if exists company.employee; create table company.employee ( empno int, ename varchar(128), age int)using com.sequoiadb.spark options( host "localhost:21810", collectionspace "company", collection "employee");'

/opt/spark/bin/beeline -u 'jdbc:hive2://localhost:10000' -e 'insert into company.employee values ( 10001, 'Georgi', 48 ),( 10002, 'Bezalel', 21 ),( 10003, 'Parto', 33 ),( 10004, 'Chirstian', 40 ),( 10005, 'Kyoichi', 23 ),( 10006, 'Anneke', 19 ),( 10007, 'Ileana', 28 ),( 10008, 'Liz', 38 ),( 10009, 'Parish', 31 ),( 10010, 'Odette', 23 );'
-->
